

import os
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime

# Modeling libs
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Plotting
import matplotlib.pyplot as plt
import seaborn as sns

# VIF helper
from statsmodels.stats.outliers_influence import variance_inflation_factor

# -------------------------
# Config / Paths
# -------------------------
DATA_PATH = Path("data") / "DS Internship - Modeling - Data.xlsx"
OUTPUT_DIR = Path("outputs")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
RANDOM_STATE = 42
TARGET_COL = "Sales"   # as described: average monthly sales (2019)

# -------------------------
# Load data
# -------------------------
df = pd.read_excel(/Users/pragnyamrinalinijakkamsetti/Downloads/Modeling - Assignments/Data/DS Internship - Modeling - Data.xlsx, sheet_name=0)
df.columns = df.columns.str.strip()  # normalize

# -------------------------
# Basic cleaning & feature engineering
# -------------------------
# Drop exact duplicate rows if any
df = df.drop_duplicates().reset_index(drop=True)

# Ensure target exists
if TARGET_COL not in df.columns:
    raise KeyError(f"Target column '{TARGET_COL}' not found in dataset columns: {df.columns.tolist()}")

# Convert Sales to numeric
df[TARGET_COL] = pd.to_numeric(df[TARGET_COL], errors="coerce")

# Convert date columns if present
# Create store age (in years) at 2019 (sales year)
REF_YEAR = 2019
if "Store Open" in df.columns:
    # some rows might be just year or string like '1991-03-01'
    df["Store_Open_dt"] = pd.to_datetime(df["Store Open"], errors="coerce")
    df["Store_Open_Year"] = df["Store_Open_dt"].dt.year
    df["Store_Age_2019"] = REF_YEAR - df["Store_Open_Year"]
else:
    df["Store_Age_2019"] = np.nan

# Store modification recency if available
if "Store Modification Date" in df.columns:
    df["Store_Mod_dt"] = pd.to_datetime(df["Store Modification Date"], errors="coerce")
    # years since modification up to 2019
    df["Years_Since_Mod_2019"] = REF_YEAR - df["Store_Mod_dt"].dt.year
else:
    df["Years_Since_Mod_2019"] = np.nan

# If "Total Sq Ft" has different name, try to robustly find it
sqft_col = None
for c in df.columns:
    if "sq" in c.lower() and "ft" in c.lower():
        sqft_col = c
        break
if sqft_col:
    df.rename(columns={sqft_col: "Total_SqFt"}, inplace=True)
else:
    df["Total_SqFt"] = np.nan

# Identify candidate features:
# We will drop identifier-like columns
drop_like = ["Store", "Year", "Month", "Store Open", "Store Open dt", "Store_Open_dt",
             "Store Close", "Store Close Parsed", "Store_Mod_dt", "Store Modification Date",
             "Store Modification", "Store Modification Date Parsed"]
drop_cols = [c for c in drop_like if c in df.columns]
# keep only useful columns plus engineered ones
candidate_df = df.copy()

# Identify categorical/object columns to dummy-encode (except target)
cat_cols = candidate_df.select_dtypes(include=["object", "category"]).columns.tolist()
# exclude target if present among them
if TARGET_COL in cat_cols:
    cat_cols.remove(TARGET_COL)
# We don't want to dummy-encode natural numeric strings that represent numbers: attempt to detect
# But we will use straightforward approach: drop common non-feature string columns
non_feature_strings = ["Store", "Store Open", "Store Close", "Store Modification", "Store Modification Date"]
cat_cols = [c for c in cat_cols if c not in non_feature_strings]

# Keep numeric columns
num_cols = candidate_df.select_dtypes(include=[np.number]).columns.tolist()
# Remove target from num_cols
num_cols = [c for c in num_cols if c != TARGET_COL]

# Feature set: choose numeric engineered features + existing numeric columns + dummies for categorical
# But remove columns that are not features (e.g., SGM if it's an id) - heuristic: if unique values >> rows treat as ID
to_exclude = []
for c in candidate_df.columns:
    if candidate_df[c].dtype == object:
        # If very high cardinality, exclude (likely an ID column like SGM names)
        if candidate_df[c].nunique() > len(candidate_df) * 0.5:
            to_exclude.append(c)

# Final categorical columns to encode
cat_cols = [c for c in cat_cols if c not in to_exclude]

# Create DataFrame X with cleaned features
X = candidate_df.copy()

# Drop obvious non-feature cols
for c in drop_cols + to_exclude:
    if c in X.columns:
        X = X.drop(columns=c)

# Keep target separately
y = X[TARGET_COL].copy()
X = X.drop(columns=[TARGET_COL])

# Fill numeric NaNs with median (simple approach)
num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
for c in num_cols:
    X[c] = pd.to_numeric(X[c], errors="coerce")
    if X[c].isna().any():
        med = X[c].median()
        X[c] = X[c].fillna(med)

# Categorical dummies (drop_first to avoid collinearity)
X = pd.get_dummies(X, columns=cat_cols, drop_first=True)

# Ensure no columns with infinite or NaN values remaining
X = X.replace([np.inf, -np.inf], np.nan).fillna(0)

# -------------------------
# Train-test split
# -------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)

# -------------------------
# Helper: compute VIF
# -------------------------
def compute_vif(df_in):
    vif_data = []
    cols = df_in.columns
    # add constant if needed
    for i in range(len(cols)):
        try:
            vif = variance_inflation_factor(df_in.values, i)
        except Exception:
            vif = np.nan
        vif_data.append((cols[i], vif))
    vif_df = pd.DataFrame(vif_data, columns=["feature", "VIF"]).sort_values("VIF", ascending=False)
    return vif_df

# Compute initial VIF on training set (scale if necessary)
vif_initial = compute_vif(X_train)

# -------------------------
# Feature selection: iterative VIF-based and p-value backward elimination
# Strategy:
# 1. Remove features with extremely high VIF (> 50) iteratively
# 2. Fit OLS on remaining features and remove highest p-value > 0.05 iteratively (backward elimination)
# -------------------------
X_train_sel = X_train.copy()
X_test_sel = X_test.copy()

# 1) VIF cleanup: iteratively drop features with VIF > 50 (aggressive threshold to remove perfect multicollinearity)
vif_threshold_high = 50.0
max_iter_vif = 10
for _ in range(max_iter_vif):
    vif_df = compute_vif(X_train_sel)
    high_vif = vif_df[vif_df["VIF"] > vif_threshold_high]
    if high_vif.empty:
        break
    # drop the highest VIF feature
    feature_to_drop = high_vif.iloc[0]["feature"]
    print(f"Dropping high-VIF feature: {feature_to_drop} (VIF={high_vif.iloc[0]['VIF']:.1f})")
    X_train_sel = X_train_sel.drop(columns=[feature_to_drop])
    X_test_sel = X_test_sel.drop(columns=[feature_to_drop], errors="ignore")

# 2) Backward elimination by p-value (OLS on train)
def backward_elimination(X_df, y_series, p_threshold=0.05, max_iter=50):
    X_work = X_df.copy()
    X_work_const = sm.add_constant(X_work)
    for i in range(max_iter):
        model = sm.OLS(y_series, X_work_const).fit()
        pvals = model.pvalues.drop("const", errors="ignore")
        if pvals.empty:
            break
        worst_p = pvals.max()
        if worst_p > p_threshold:
            worst_feature = pvals.idxmax()
            print(f"Eliminating feature '{worst_feature}' with p-value {worst_p:.4f}")
            # drop it from both X_work_const and underlying X_work
            X_work = X_work.drop(columns=[worst_feature])
            X_work_const = sm.add_constant(X_work)
        else:
            break
    return X_work, model

X_train_final, ols_model = backward_elimination(X_train_sel, y_train, p_threshold=0.05, max_iter=200)

# Align test columns with selected train columns
X_test_final = X_test_sel.reindex(columns=X_train_final.columns).fillna(0)

# -------------------------
# Refit final model on TRAIN using statsmodels (final inference)
# -------------------------
X_train_final_const = sm.add_constant(X_train_final)
final_model = sm.OLS(y_train, X_train_final_const).fit()

# Save statsmodels summary
with open(OUTPUT_DIR / "model_summary.txt", "w") as f:
    f.write(final_model.summary().as_text())

# -------------------------
# Predictions & performance on TEST
# -------------------------
X_test_final_const = sm.add_constant(X_test_final, has_constant='add')
y_pred_test = final_model.predict(X_test_final_const)
y_pred_train = final_model.predict(X_train_final_const)

# Metrics functions
def mape(y_true, y_pred):
    # Avoid division by zero: use small epsilon for zeros
    eps = 1e-9
    y_true_safe = np.where(y_true == 0, eps, y_true)
    return np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100

def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

perf = {
    "train_R2": r2_score(y_train, y_pred_train),
    "test_R2": r2_score(y_test, y_pred_test),
    "train_MAE": mean_absolute_error(y_train, y_pred_train),
    "test_MAE": mean_absolute_error(y_test, y_pred_test),
    "train_RMSE": rmse(y_train, y_pred_train),
    "test_RMSE": rmse(y_test, y_pred_test),
    "train_MAPE": mape(np.array(y_train), np.array(y_pred_train)),
    "test_MAPE": mape(np.array(y_test), np.array(y_pred_test)),
}

# -------------------------
# Residual analysis
# -------------------------
residuals_test = y_test - y_pred_test
residuals_train = y_train - y_pred_train

# Plot residuals vs predicted (test)
plt.figure(figsize=(8,6))
sns.scatterplot(x=y_pred_test, y=residuals_test, alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel("Predicted Sales (test)")
plt.ylabel("Residuals (y_true - y_pred)")
plt.title("Residuals vs Predicted (Test)")
plt.tight_layout()
plt.savefig(OUTPUT_DIR / "residuals_vs_pred_test.png")
plt.close()

# Plot predicted vs actual (test)
plt.figure(figsize=(7,7))
sns.scatterplot(x=y_test, y=y_pred_test, alpha=0.6)
lims = [min(y_test.min(), y_pred_test.min()), max(y_test.max(), y_pred_test.max())]
plt.plot(lims, lims, '--', color='red')
plt.xlabel("Actual Sales (test)")
plt.ylabel("Predicted Sales (test)")
plt.title("Predicted vs Actual Sales (Test)")
plt.tight_layout()
plt.savefig(OUTPUT_DIR / "predicted_vs_actual_test.png")
plt.close()

# QQ plot for residuals (test)
fig = sm.qqplot(residuals_test, line='s')
fig.savefig(OUTPUT_DIR / "qqplot_residuals_test.png")
plt.close()

# -------------------------
# VIF of final features
# -------------------------
vif_final = compute_vif(X_train_final)
# keep top N vif sorted
vif_final_sorted = vif_final.sort_values("VIF", ascending=False)

# -------------------------
# Save outputs to Excel
# -------------------------
coef_df = pd.DataFrame({
    "feature": final_model.params.index,
    "coefficient": final_model.params.values,
    "p_value": final_model.pvalues,
    "std_err": final_model.bse
}).reset_index(drop=True)

perf_df = pd.DataFrame([perf])
vif_df_out = vif_final_sorted

# A small sample of residuals
resid_sample = pd.DataFrame({
    "store_index": y_test.index,
    "actual": y_test.values,
    "predicted": y_pred_test,
    "residual": residuals_test
}).reset_index(drop=True)

# Write to Excel with multiple sheets
out_excel = OUTPUT_DIR / "model_results.xlsx"
with pd.ExcelWriter(out_excel, engine="openpyxl") as writer:
    coef_df.to_excel(writer, sheet_name="coefficients", index=False)
    perf_df.to_excel(writer, sheet_name="performance", index=False)
    vif_df_out.to_excel(writer, sheet_name="vif", index=False)
    resid_sample.to_excel(writer, sheet_name="residuals_sample", index=False)

# Also write CSV copies
coef_df.to_csv(OUTPUT_DIR / "coefficients.csv", index=False)
perf_df.to_csv(OUTPUT_DIR / "performance.csv", index=False)
vif_df_out.to_csv(OUTPUT_DIR / "vif.csv", index=False)
resid_sample.to_csv(OUTPUT_DIR / "residuals_sample.csv", index=False)

# -------------------------
# Print summary results to console
# -------------------------
print("\n=== MODEL PERFORMANCE METRICS ===")
for k, v in perf.items():
    if isinstance(v, float):
        print(f"{k}: {v:.4f}")
    else:
        print(f"{k}: {v}")

print("\nFinal model saved to:", out_excel)
print("Model summary text saved to:", OUTPUT_DIR / "model_summary.txt")
print("Residual & diagnostic plots saved to outputs/ (png files)")
